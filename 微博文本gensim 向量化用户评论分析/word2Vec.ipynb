{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e494565f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.char'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjieba\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#import gensim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mword2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LineSentence\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/parsing/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      5\u001b[39m     preprocess_documents,\n\u001b[32m      6\u001b[39m     preprocess_string,\n\u001b[32m      7\u001b[39m     read_file,\n\u001b[32m      8\u001b[39m     read_files,\n\u001b[32m      9\u001b[39m     remove_stopwords,\n\u001b[32m     10\u001b[39m     split_alphanum,\n\u001b[32m     11\u001b[39m     stem_text,\n\u001b[32m     12\u001b[39m     strip_multiple_whitespaces,\n\u001b[32m     13\u001b[39m     strip_non_alphanum,\n\u001b[32m     14\u001b[39m     strip_numeric,\n\u001b[32m     15\u001b[39m     strip_punctuation,\n\u001b[32m     16\u001b[39m     strip_short,\n\u001b[32m     17\u001b[39m     strip_tags,\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/parsing/preprocessing.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[32m     30\u001b[39m STOPWORDS = \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjust\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mless\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbeing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mindeed\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mover\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmove\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33manyway\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnot\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mown\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mthrough\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33musing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfifty\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhere\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmill\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monly\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfind\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbefore\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhose\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msomewhere\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmake\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monce\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     59\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/utils.py:35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msmart_open\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/sparse/__init__.py:294\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/sparse/_base.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      8\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      9\u001b[39m                        matrix, validateaxis,)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/_lib/_util.py:18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     Optional,\n\u001b[32m     12\u001b[39m     Union,\n\u001b[32m     13\u001b[39m     TYPE_CHECKING,\n\u001b[32m     14\u001b[39m     TypeVar,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[32m     21\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[32m     22\u001b[39m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/_lib/_array_api.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     is_array_api_obj,\n\u001b[32m     19\u001b[39m     size,\n\u001b[32m     20\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/numpy/__init__.py:370\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    365\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    367\u001b[39m     msg = (\u001b[33m\"\u001b[39m\u001b[33mThe current Numpy installation (\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m) fails to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mpass simple sanity checks. This can be caused for example \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mby incorrect BLAS library being linked in, or by mixing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mpackage managers (pip, conda, apt, ...). Search closed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mnumpy issues for similar problems.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg.format(\u001b[34m__file__\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy.char'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "#import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92917af",
   "metadata": {},
   "source": [
    "### 一\n",
    "定义一个类TextAnalyzer，其属性包括待分析的文本文件路径，等加载的预训练模型文件路径，训练word2vec的一些简单参数\n",
    "（如向量长度，窗口大小）等，初始化的时候需要对这些属性进行定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee5b8a",
   "metadata": {},
   "source": [
    "### 二\n",
    "在上述类加入一个预处理方法_pre_process，如将待分析的weibo.txt加载到内存（会先解压weibo.txt.zip)，进行基本的文本预处理，如对所有微博内容进行去重，进行分词、去除停用词、标点等，最终建立一个以微博为单位进行分词的二维列表。\n",
    "\n",
    "_！weibo.txt一行为一条微博的属性，用\\t分隔后，第二个元素为微博内容。（提供的weibo.txt包含大量重复和标点等，需要仔细预处理，否则会影响后面的嵌入模型训练。）_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cc216",
   "metadata": {},
   "source": [
    "### 三\n",
    "#在上述类加入一个方法_get_word2vec_model来利用2中构建的微博二维列表来训练Word2Vec模型，可参照demo使用gensim完成Word2Vec模型的建立。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd31ee",
   "metadata": {},
   "source": [
    "### 四\n",
    "上述类加入一个方法get_similar_words来利用训练得到的word2vec模型来推断相似词汇。如输入一个任意词，使用model.similarity方法可以来返回与目标词汇相近的一定数目的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed28212",
   "metadata": {},
   "source": [
    "### 五\n",
    "#词向量能够将每个词定位为高维空间中的一个点，且不同词间的“差异”可以通过点间的距离来反应。在类加再增加一个方法`vis_word_tsne`\n",
    "\n",
    "使用TSNE算法，对与某个输入词的最相关以及最不相关的词语（用参数来控制数目）进行降维和可视化。    \n",
    "    def vis_word_tsne(self,input_word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57819bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    # 一 --------------------------------------------------------------\n",
    "    # 配置日志，特别是在处理文件和训练模型时候显示进度\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    def __init__(self, file_path, model_path, len_vec, size_window, cache_path = \"processed_words.txt\"):\n",
    "        self.cache_path = cache_path\n",
    "        self.file_path = file_path\n",
    "        self.model_path = model_path\n",
    "        self.len_vec = len_vec\n",
    "        self.size_window = size_window\n",
    "        self.processed_words = None\n",
    "        self.cache_path = cache_path # 用于文件预处理的文件储存，第二次跑的时候可以直接使用不需要重复处理；同时可以查看处理结果不需要打印，以便优化过滤逻辑\n",
    "        \n",
    "    def _remove_emojis(self, text): # 移除emoji，不需要手动\n",
    "        return emoji.replace_emoji(text,replace='')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 二 --------------------------------------------------------------\n",
    "    def _pre_process(self): # 文件预处理成一个txt文本，方便训练模型直接使用\n",
    "        words_path = self.cache_path\n",
    "        # 免去处理已有文件，一次处理\n",
    "        if os.path.exists(self.cache_path):\n",
    "            return words_path\n",
    "\n",
    "        stopset = set([  # 用集合加速查询\n",
    "            '，','【','】','#','s','1','2','3','4','5','6','7','8','9','MIUI','@','_','(',')','￣','！','[',']','-',\n",
    "            '“','”','p','ID',':','：','~','td','t','d','//','/','？','。','%','>>','>','《','》','、','+','ing','TNND','〜',\n",
    "            '🎂','⊙','o','(⊙o⊙)','⊙o⊙','.','..','..........','℃','H','……','^','*','$','￥','↗','↖','ω','↖(^ω^)↗','😃','…','\"\"','\"','Y',\n",
    "            'http','cn','com','!',',','的','了','～','...','在','.','；',';','◆','「','」','##','?','∀','·','ｲ','&','---',',','≧','▽','≦','`',\n",
    "            '(',')','（','）'\n",
    "        ])\n",
    "\n",
    "        chunks = pd.read_csv(\n",
    "            self.file_path,\n",
    "            sep='\\t',\n",
    "            header=0,\n",
    "            names=['text'],\n",
    "            chunksize=10000,\n",
    "            usecols=[1],\n",
    "            dtype={'text': str},\n",
    "            engine='c',\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "\n",
    "        words = []\n",
    "        jieba.enable_parallel(4)  # 使用 4 个 CPU 核心\n",
    "        for chunk in tqdm(chunks, desc=\"Processing Text\"):\n",
    "            texts = set(chunk['text'].dropna())  # 用 set() 代替 unique()\n",
    "            # 除去url 和表情符\n",
    "            cleaned_texts = []\n",
    "            for text in texts:\n",
    "                # 去除URL\n",
    "                text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "                # 去除特殊符号和表情符号\n",
    "                text = re.sub(r'\\[.*?\\]', '', text)\n",
    "                # 去除emoji\n",
    "                text = self._remove_emojis(text)\n",
    "                cleaned_texts.append(text)\n",
    "            words.extend([\n",
    "                [w for w in jieba.lcut(str(text)) if w not in stopset]\n",
    "                for text in cleaned_texts\n",
    "            ])\n",
    "        with open(self.cache_path, 'w', encoding='utf-8')as f:\n",
    "            for sentence in words:\n",
    "                if not sentence:  # 跳过空句子\n",
    "                    continue\n",
    "                f.write(' '.join(sentence)+'\\n')\n",
    "        print(\"Cache file saved.\")        \n",
    "        self.processed_words = words\n",
    "        return words_path # cache_path   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 三 --------------------------------------------------------------\n",
    "    def _get_word2vec_model(self):\n",
    "        if os.path.exists(self.model_path):\n",
    "            print(f\"Loading existing model from {self.model_path}\")\n",
    "            return Word2Vec.load(self.model_path)\n",
    "        \n",
    "        words_path = self._pre_process()\n",
    "        if not words_path:  # 避免训练空模型\n",
    "            raise ValueError(\"No valid words to train the model.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"开始训练模型...\")\n",
    "        \n",
    "        # 使用LineSentence读取大型文本文件(逐行读取，避免内存不足)\n",
    "        sentences = LineSentence(words_path)\n",
    "        \n",
    "        model = Word2Vec(\n",
    "            #list(tqdm(words_list, desc=\"Training Word2Vec\")),#使用 tqdm 显示 Word2Vec 训练进度\n",
    "            #sentences=tqdm(words_list,desc=\"Training Word2Vec\", total=len(words_list)),\n",
    "            sentences=sentences,\n",
    "            vector_size=self.len_vec,\n",
    "            window=self.size_window,\n",
    "            min_count=5,\n",
    "            workers=8,          # 使用多线程\n",
    "            sg=1,               # 使用skip-gram模型(1), 0表示CBOW\n",
    "            epochs=50,          # 可选：设置训练轮数10    \n",
    "            #compute_loss=True,  # 可选：显示训练损失\n",
    "            hs=0,               # 使用负采样\n",
    "            negative=5,          # 负采样数量\n",
    "            sample=1e-3         # 高频词下采样阈值\n",
    "        )\n",
    "        model.save(self.model_path)\n",
    "        # 保存词向量(纯文本格式，可用于其他工具)\n",
    "        model.wv.save_word2vec_format('/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.vec', binary=False)\n",
    "        \n",
    "        end_time = time.time() # 时间戳\n",
    "        print(f\"训练完成! 耗时: {end_time-start_time:.2f}秒\")\n",
    "        print(f\"模型已保存至: {self.model_path}\")\n",
    "        print(f\"词向量已保存至: /Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.vec\")\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 四 --------------------------------------------------------------\n",
    "    def get_similar_words(self, word):\n",
    "        model = self._get_word2vec_model()\n",
    "        return model.wv.most_similar(word, topn=10)\n",
    "    def get_least_similar_words(self,word):\n",
    "        model = self._get_word2vec_model()\n",
    "        return model.wv.most_similar(word, topn=10, negative=word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 五 --------------------------------------------------------------\n",
    "    def vis_word_tsne(self,input_word):\n",
    "        model=self._get_word2vec_model()\n",
    "        most_similar = model.wv.most_similar(input_word,topn=10)\n",
    "        least_similar = model.wv.most_similar(input_word,topn=10,negative=input_word)\n",
    "        \n",
    "        #       np.array([model.wv[word] for word, similarity in most_similar + least_similar])\n",
    "        vectors=np.array([model.wv[w]for w,similarity in most_similar+least_similar])\n",
    "        w_list = [w for w, similarity in most_similar+least_similar]\n",
    "        # 使用t-SNE算法对词向量进行降维\n",
    "        tsne = TSNE(n_components=2, perplexity=15)\n",
    "        vectors_tsne = tsne.fit_transform(vectors)\n",
    "        # 可视化降维后的词向量\n",
    "        #fonts\n",
    "        STH = FontProperties(fname='/System/Library/Fonts/STHeiti Light.ttc')\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(word, fontproperties = STH)\n",
    "        # t-SNE降维至2维\n",
    "        ax.scatter(vectors_tsne[:10, 0], vectors_tsne[:10, 1], color='purple',label = 'most_simi')\n",
    "        ax.scatter(vectors_tsne[10:, 0], vectors_tsne[10:, 1], color='orange', label = 'least_simi')\n",
    "        ax.legend()\n",
    "        # 打印词\n",
    "        for i, w in enumerate(w_list):\n",
    "            ax.annotate(w, (vectors_tsne[i, 0], vectors_tsne[i, 1]),fontproperties = STH)\n",
    "        plt.show()    \n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text_ana = TextAnalyzer(\n",
    "        file_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo.txt',\n",
    "        model_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.model',\n",
    "        len_vec=200,\n",
    "        size_window=5,\n",
    "        cache_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/processed_words.txt'\n",
    "    )\n",
    "    word='旅行'\n",
    "    print(f\"和{word}最相似的10个词：\",text_ana.get_similar_words(word))\n",
    "    print(f\"和{word}最不相似的10个词：\",text_ana.get_least_similar_words(word))\n",
    "    text_ana.vis_word_tsne(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import pandas as pd
import numpy as np
import jieba
#import gensim
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
from tqdm import tqdm
import os
import re
import emoji
import io
import logging
import time
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
###ä¸€########################################################################################
#å®šä¹‰ä¸€ä¸ªç±»TextAnalyzerï¼Œå…¶å±æ€§åŒ…æ‹¬å¾…åˆ†æçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œç­‰åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œè®­ç»ƒword2vecçš„ä¸€äº›ç®€å•å‚æ•°
# ï¼ˆå¦‚å‘é‡é•¿åº¦ï¼Œçª—å£å¤§å°ï¼‰ç­‰ï¼Œåˆå§‹åŒ–çš„æ—¶å€™éœ€è¦å¯¹è¿™äº›å±æ€§è¿›è¡Œå®šä¹‰ã€‚

class TextAnalyzer:
    # é…ç½®æ—¥å¿—ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ–‡ä»¶å’Œè®­ç»ƒæ¨¡å‹æ—¶å€™æ˜¾ç¤ºè¿›åº¦
    #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    def __init__(self, file_path, model_path, len_vec, size_window, cache_path = "processed_words.txt"):
        self.cache_path = cache_path
        self.file_path = file_path
        self.model_path = model_path
        self.len_vec = len_vec
        self.size_window = size_window
        self.processed_words = None
        self.cache_path = cache_path # ç”¨äºæ–‡ä»¶é¢„å¤„ç†çš„æ–‡ä»¶å‚¨å­˜ï¼Œç¬¬äºŒæ¬¡è·‘çš„æ—¶å€™å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸éœ€è¦é‡å¤å¤„ç†ï¼›åŒæ—¶å¯ä»¥æŸ¥çœ‹å¤„ç†ç»“æœä¸éœ€è¦æ‰“å°ï¼Œä»¥ä¾¿ä¼˜åŒ–è¿‡æ»¤é€»è¾‘
        
    def _remove_emojis(self, text): # ç§»é™¤emojiï¼Œä¸éœ€è¦æ‰‹åŠ¨
        return emoji.replace_emoji(text,replace='')

###äºŒ########################################################################################
#åœ¨ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªé¢„å¤„ç†æ–¹æ³•_pre_processï¼Œå¦‚å°†å¾…åˆ†æçš„weibo.txtåŠ è½½åˆ°å†…å­˜ï¼ˆè¯·å…ˆè§£å‹æä¾›çš„weibo.txt.zip)ï¼Œè¿›è¡ŒåŸºæœ¬çš„æ–‡æœ¬é¢„å¤„ç†ï¼Œ
# å¦‚å¯¹æ‰€æœ‰å¾®åšå†…å®¹è¿›è¡Œå»é‡ï¼Œè¿›è¡Œåˆ†è¯ã€å»é™¤åœç”¨è¯ã€æ ‡ç‚¹ç­‰ï¼Œæœ€ç»ˆå»ºç«‹ä¸€ä¸ªä»¥å¾®åšä¸ºå•ä½è¿›è¡Œåˆ†è¯çš„äºŒç»´åˆ—è¡¨ã€‚æ³¨æ„ï¼Œweibo.txtä¸€è¡Œä¸ºä¸€æ¡å¾®åšçš„å±æ€§ï¼Œç”¨\tåˆ†éš”åï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºå¾®åšå†…å®¹ã€‚ï¼ˆæä¾›çš„weibo.txtåŒ…å«å¤§é‡é‡å¤å’Œæ ‡ç‚¹ç­‰ï¼Œéœ€è¦ä»”ç»†é¢„å¤„ç†ï¼Œå¦åˆ™ä¼šå½±å“åé¢çš„åµŒå…¥æ¨¡å‹è®­ç»ƒã€‚ï¼‰
    def _pre_process(self): # æ–‡ä»¶é¢„å¤„ç†æˆä¸€ä¸ªtxtæ–‡æœ¬ï¼Œæ–¹ä¾¿è®­ç»ƒæ¨¡å‹ç›´æ¥ä½¿ç”¨
        words_path = self.cache_path
        # å…å»å¤„ç†å·²æœ‰æ–‡ä»¶ï¼Œä¸€æ¬¡å¤„ç†
        if os.path.exists(self.cache_path):
            return words_path

        stopset = set([  # ç”¨é›†åˆåŠ é€ŸæŸ¥è¯¢
            'ï¼Œ','ã€','ã€‘','#','s','1','2','3','4','5','6','7','8','9','MIUI','@','_','(',')','ï¿£','ï¼','[',']','-',
            'â€œ','â€','p','ID',':','ï¼š','~','td','t','d','//','/','ï¼Ÿ','ã€‚','%','>>','>','ã€Š','ã€‹','ã€','+','ing','TNND','ã€œ',
            'ğŸ‚','âŠ™','o','(âŠ™oâŠ™)','âŠ™oâŠ™','.','..','..........','â„ƒ','H','â€¦â€¦','^','*','$','ï¿¥','â†—','â†–','Ï‰','â†–(^Ï‰^)â†—','ğŸ˜ƒ','â€¦','""','"','Y',
            'http','cn','com','!',',','çš„','äº†','ï½','...','åœ¨','.','ï¼›',';','â—†','ã€Œ','ã€','##','?','âˆ€','Â·','ï½²','&','---',',','â‰§','â–½','â‰¦','`',
            '(',')','ï¼ˆ','ï¼‰'
        ])

        chunks = pd.read_csv(
            self.file_path,
            sep='\t',
            header=0,
            names=['text'],
            chunksize=10000,
            usecols=[1],
            dtype={'text': str},
            engine='c',
            on_bad_lines='skip'
        )

        words = []
        jieba.enable_parallel(4)  # ä½¿ç”¨ 4 ä¸ª CPU æ ¸å¿ƒ
        for chunk in tqdm(chunks, desc="Processing Text"):
            texts = set(chunk['text'].dropna())  # ç”¨ set() ä»£æ›¿ unique()
            # é™¤å»url å’Œè¡¨æƒ…ç¬¦
            cleaned_texts = []
            for text in texts:
                # å»é™¤URL
                text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
                # å»é™¤ç‰¹æ®Šç¬¦å·å’Œè¡¨æƒ…ç¬¦å·
                text = re.sub(r'\[.*?\]', '', text)
                # å»é™¤emoji
                text = self._remove_emojis(text)
                cleaned_texts.append(text)
            words.extend([
                [w for w in jieba.lcut(str(text)) if w not in stopset]
                for text in cleaned_texts
            ])
        with open(self.cache_path, 'w', encoding='utf-8')as f:
            for sentence in words:
                if not sentence:  # è·³è¿‡ç©ºå¥å­
                    continue
                f.write(' '.join(sentence)+'\n')
        print("Cache file saved.")        
        self.processed_words = words
        return words_path # cache_path

###ä¸‰########################################################################################
#åœ¨ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªæ–¹æ³•_get_word2vec_modelæ¥åˆ©ç”¨2ä¸­æ„å»ºçš„å¾®åšäºŒç»´åˆ—è¡¨æ¥è®­ç»ƒWord2Vecæ¨¡å‹ï¼Œå¯å‚ç…§demoä½¿ç”¨gensimå®ŒæˆWord2Vecæ¨¡å‹çš„å»ºç«‹ã€‚
    def _get_word2vec_model(self):
        if os.path.exists(self.model_path):
            print(f"Loading existing model from {self.model_path}")
            return Word2Vec.load(self.model_path)
        
        words_path = self._pre_process()
        if not words_path:  # é¿å…è®­ç»ƒç©ºæ¨¡å‹
            raise ValueError("No valid words to train the model.")
        
        start_time = time.time()
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # ä½¿ç”¨LineSentenceè¯»å–å¤§å‹æ–‡æœ¬æ–‡ä»¶(é€è¡Œè¯»å–ï¼Œé¿å…å†…å­˜ä¸è¶³)
        sentences = LineSentence(words_path)
        
        model = Word2Vec(
            #list(tqdm(words_list, desc="Training Word2Vec")),#ä½¿ç”¨ tqdm æ˜¾ç¤º Word2Vec è®­ç»ƒè¿›åº¦
            #sentences=tqdm(words_list,desc="Training Word2Vec", total=len(words_list)),
            sentences=sentences,
            vector_size=self.len_vec,
            window=self.size_window,
            min_count=5,
            workers=8,          # ä½¿ç”¨å¤šçº¿ç¨‹
            sg=1,               # ä½¿ç”¨skip-gramæ¨¡å‹(1), 0è¡¨ç¤ºCBOW
            epochs=50,          # å¯é€‰ï¼šè®¾ç½®è®­ç»ƒè½®æ•°10    
            #compute_loss=True,  # å¯é€‰ï¼šæ˜¾ç¤ºè®­ç»ƒæŸå¤±
            hs=0,               # ä½¿ç”¨è´Ÿé‡‡æ ·
            negative=5,          # è´Ÿé‡‡æ ·æ•°é‡
            sample=1e-3         # é«˜é¢‘è¯ä¸‹é‡‡æ ·é˜ˆå€¼
        )
        model.save(self.model_path)
        # ä¿å­˜è¯å‘é‡(çº¯æ–‡æœ¬æ ¼å¼ï¼Œå¯ç”¨äºå…¶ä»–å·¥å…·)
        model.wv.save_word2vec_format('/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.vec', binary=False)
        
        end_time = time.time() # æ—¶é—´æˆ³
        print(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {end_time-start_time:.2f}ç§’")
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {self.model_path}")
        print(f"è¯å‘é‡å·²ä¿å­˜è‡³: /Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.vec")
        return model

###å››########################################################################################
#åœ¨ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªæ–¹æ³•get_similar_wordsæ¥åˆ©ç”¨è®­ç»ƒå¾—åˆ°çš„word2vecæ¨¡å‹æ¥æ¨æ–­ç›¸ä¼¼è¯æ±‡ã€‚å¦‚è¾“å…¥ä¸€ä¸ªä»»æ„è¯ï¼Œä½¿ç”¨model.similarityæ–¹æ³•å¯ä»¥æ¥è¿”å›ä¸ç›®æ ‡è¯æ±‡ç›¸è¿‘çš„ä¸€å®šæ•°ç›®çš„è¯ã€‚
    def get_similar_words(self, word):
        model = self._get_word2vec_model()
        return model.wv.most_similar(word, topn=10)
    def get_least_similar_words(self,word):
        model = self._get_word2vec_model()
        return model.wv.most_similar(word, topn=10, negative=word)

###å…­######################################################################################
#è¯å‘é‡èƒ½å¤Ÿå°†æ¯ä¸ªè¯å®šä½ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œä¸”ä¸åŒè¯é—´çš„â€œå·®å¼‚â€å¯ä»¥é€šè¿‡ç‚¹é—´çš„è·ç¦»æ¥ååº”ã€‚åœ¨ç±»åŠ å†å¢åŠ ä¸€ä¸ªæ–¹æ³•vis_word_tsne
# ä½¿ç”¨TSNEç®—æ³•ï¼Œå¯¹ä¸æŸä¸ªè¾“å…¥è¯çš„æœ€ç›¸å…³ä»¥åŠæœ€ä¸ç›¸å…³çš„è¯è¯­ï¼ˆç”¨å‚æ•°æ¥æ§åˆ¶æ•°ç›®ï¼‰è¿›è¡Œé™ç»´å’Œå¯è§†åŒ–ã€‚    
    def vis_word_tsne(self,input_word):
        model=self._get_word2vec_model()
        most_similar = model.wv.most_similar(input_word,topn=10)
        least_similar = model.wv.most_similar(input_word,topn=10,negative=input_word)
        
        #       np.array([model.wv[word] for word, similarity in most_similar + least_similar])
        vectors=np.array([model.wv[w]for w,similarity in most_similar+least_similar])
        w_list = [w for w, similarity in most_similar+least_similar]
        # ä½¿ç”¨t-SNEç®—æ³•å¯¹è¯å‘é‡è¿›è¡Œé™ç»´
        tsne = TSNE(n_components=2, perplexity=15)
        vectors_tsne = tsne.fit_transform(vectors)
        # å¯è§†åŒ–é™ç»´åçš„è¯å‘é‡
        #fonts
        STH = FontProperties(fname='/System/Library/Fonts/STHeiti Light.ttc')
        
        fig, ax = plt.subplots()
        ax.set_title(word, fontproperties = STH)
        # t-SNEé™ç»´è‡³2ç»´
        ax.scatter(vectors_tsne[:10, 0], vectors_tsne[:10, 1], color='purple',label = 'most_simi')
        ax.scatter(vectors_tsne[10:, 0], vectors_tsne[10:, 1], color='orange', label = 'least_simi')
        ax.legend()
        # æ‰“å°è¯
        for i, w in enumerate(w_list):
            ax.annotate(w, (vectors_tsne[i, 0], vectors_tsne[i, 1]),fontproperties = STH)
        plt.show()

if __name__ == "__main__":
    text_ana = TextAnalyzer(
        file_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo.txt',
        model_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.model',
        len_vec=200,
        size_window=5,
        cache_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/processed_words.txt'
    )
    word='æ—…è¡Œ'
    print(f"å’Œ{word}æœ€ç›¸ä¼¼çš„10ä¸ªè¯ï¼š",text_ana.get_similar_words(word))
    print(f"å’Œ{word}æœ€ä¸ç›¸ä¼¼çš„10ä¸ªè¯ï¼š",text_ana.get_least_similar_words(word))
    text_ana.vis_word_tsne(word)
    

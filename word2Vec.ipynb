{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e494565f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.char'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjieba\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#import gensim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mword2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LineSentence\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/parsing/__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      5\u001b[39m     preprocess_documents,\n\u001b[32m      6\u001b[39m     preprocess_string,\n\u001b[32m      7\u001b[39m     read_file,\n\u001b[32m      8\u001b[39m     read_files,\n\u001b[32m      9\u001b[39m     remove_stopwords,\n\u001b[32m     10\u001b[39m     split_alphanum,\n\u001b[32m     11\u001b[39m     stem_text,\n\u001b[32m     12\u001b[39m     strip_multiple_whitespaces,\n\u001b[32m     13\u001b[39m     strip_non_alphanum,\n\u001b[32m     14\u001b[39m     strip_numeric,\n\u001b[32m     15\u001b[39m     strip_punctuation,\n\u001b[32m     16\u001b[39m     strip_short,\n\u001b[32m     17\u001b[39m     strip_tags,\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/parsing/preprocessing.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstring\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mporter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[32m     30\u001b[39m STOPWORDS = \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjust\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mless\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbeing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mindeed\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mover\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmove\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33manyway\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnot\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mown\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mthrough\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33musing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfifty\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhere\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmill\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monly\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfind\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbefore\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mone\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwhose\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msomewhere\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmake\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monce\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     59\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/gensim/utils.py:35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msmart_open\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/sparse/__init__.py:294\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/sparse/_base.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      8\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      9\u001b[39m                        matrix, validateaxis,)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/_lib/_util.py:18\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     Optional,\n\u001b[32m     12\u001b[39m     Union,\n\u001b[32m     13\u001b[39m     TYPE_CHECKING,\n\u001b[32m     14\u001b[39m     TypeVar,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[32m     21\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[32m     22\u001b[39m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/_lib/_array_api.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     is_array_api_obj,\n\u001b[32m     19\u001b[39m     size,\n\u001b[32m     20\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlpenv/lib/python3.12/site-packages/numpy/__init__.py:370\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    365\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    367\u001b[39m     msg = (\u001b[33m\"\u001b[39m\u001b[33mThe current Numpy installation (\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m) fails to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mpass simple sanity checks. This can be caused for example \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mby incorrect BLAS library being linked in, or by mixing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mpackage managers (pip, conda, apt, ...). Search closed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mnumpy issues for similar problems.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg.format(\u001b[34m__file__\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy.char'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "#import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92917af",
   "metadata": {},
   "source": [
    "### ä¸€\n",
    "å®šä¹‰ä¸€ä¸ªç±»TextAnalyzerï¼Œå…¶å±æ€§åŒ…æ‹¬å¾…åˆ†æçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œç­‰åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œè®­ç»ƒword2vecçš„ä¸€äº›ç®€å•å‚æ•°\n",
    "ï¼ˆå¦‚å‘é‡é•¿åº¦ï¼Œçª—å£å¤§å°ï¼‰ç­‰ï¼Œåˆå§‹åŒ–çš„æ—¶å€™éœ€è¦å¯¹è¿™äº›å±æ€§è¿›è¡Œå®šä¹‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee5b8a",
   "metadata": {},
   "source": [
    "### äºŒ\n",
    "åœ¨ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªé¢„å¤„ç†æ–¹æ³•_pre_processï¼Œå¦‚å°†å¾…åˆ†æçš„weibo.txtåŠ è½½åˆ°å†…å­˜ï¼ˆä¼šå…ˆè§£å‹weibo.txt.zip)ï¼Œè¿›è¡ŒåŸºæœ¬çš„æ–‡æœ¬é¢„å¤„ç†ï¼Œå¦‚å¯¹æ‰€æœ‰å¾®åšå†…å®¹è¿›è¡Œå»é‡ï¼Œè¿›è¡Œåˆ†è¯ã€å»é™¤åœç”¨è¯ã€æ ‡ç‚¹ç­‰ï¼Œæœ€ç»ˆå»ºç«‹ä¸€ä¸ªä»¥å¾®åšä¸ºå•ä½è¿›è¡Œåˆ†è¯çš„äºŒç»´åˆ—è¡¨ã€‚\n",
    "\n",
    "_ï¼weibo.txtä¸€è¡Œä¸ºä¸€æ¡å¾®åšçš„å±æ€§ï¼Œç”¨\\tåˆ†éš”åï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºå¾®åšå†…å®¹ã€‚ï¼ˆæä¾›çš„weibo.txtåŒ…å«å¤§é‡é‡å¤å’Œæ ‡ç‚¹ç­‰ï¼Œéœ€è¦ä»”ç»†é¢„å¤„ç†ï¼Œå¦åˆ™ä¼šå½±å“åé¢çš„åµŒå…¥æ¨¡å‹è®­ç»ƒã€‚ï¼‰_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cc216",
   "metadata": {},
   "source": [
    "### ä¸‰\n",
    "#åœ¨ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªæ–¹æ³•_get_word2vec_modelæ¥åˆ©ç”¨2ä¸­æ„å»ºçš„å¾®åšäºŒç»´åˆ—è¡¨æ¥è®­ç»ƒWord2Vecæ¨¡å‹ï¼Œå¯å‚ç…§demoä½¿ç”¨gensimå®ŒæˆWord2Vecæ¨¡å‹çš„å»ºç«‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd31ee",
   "metadata": {},
   "source": [
    "### å››\n",
    "ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªæ–¹æ³•get_similar_wordsæ¥åˆ©ç”¨è®­ç»ƒå¾—åˆ°çš„word2vecæ¨¡å‹æ¥æ¨æ–­ç›¸ä¼¼è¯æ±‡ã€‚å¦‚è¾“å…¥ä¸€ä¸ªä»»æ„è¯ï¼Œä½¿ç”¨model.similarityæ–¹æ³•å¯ä»¥æ¥è¿”å›ä¸ç›®æ ‡è¯æ±‡ç›¸è¿‘çš„ä¸€å®šæ•°ç›®çš„è¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed28212",
   "metadata": {},
   "source": [
    "### äº”\n",
    "#è¯å‘é‡èƒ½å¤Ÿå°†æ¯ä¸ªè¯å®šä½ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œä¸”ä¸åŒè¯é—´çš„â€œå·®å¼‚â€å¯ä»¥é€šè¿‡ç‚¹é—´çš„è·ç¦»æ¥ååº”ã€‚åœ¨ç±»åŠ å†å¢åŠ ä¸€ä¸ªæ–¹æ³•`vis_word_tsne`\n",
    "\n",
    "ä½¿ç”¨TSNEç®—æ³•ï¼Œå¯¹ä¸æŸä¸ªè¾“å…¥è¯çš„æœ€ç›¸å…³ä»¥åŠæœ€ä¸ç›¸å…³çš„è¯è¯­ï¼ˆç”¨å‚æ•°æ¥æ§åˆ¶æ•°ç›®ï¼‰è¿›è¡Œé™ç»´å’Œå¯è§†åŒ–ã€‚    \n",
    "    def vis_word_tsne(self,input_word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57819bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    # ä¸€ --------------------------------------------------------------\n",
    "    # é…ç½®æ—¥å¿—ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ–‡ä»¶å’Œè®­ç»ƒæ¨¡å‹æ—¶å€™æ˜¾ç¤ºè¿›åº¦\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    def __init__(self, file_path, model_path, len_vec, size_window, cache_path = \"processed_words.txt\"):\n",
    "        self.cache_path = cache_path\n",
    "        self.file_path = file_path\n",
    "        self.model_path = model_path\n",
    "        self.len_vec = len_vec\n",
    "        self.size_window = size_window\n",
    "        self.processed_words = None\n",
    "        self.cache_path = cache_path # ç”¨äºæ–‡ä»¶é¢„å¤„ç†çš„æ–‡ä»¶å‚¨å­˜ï¼Œç¬¬äºŒæ¬¡è·‘çš„æ—¶å€™å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸éœ€è¦é‡å¤å¤„ç†ï¼›åŒæ—¶å¯ä»¥æŸ¥çœ‹å¤„ç†ç»“æœä¸éœ€è¦æ‰“å°ï¼Œä»¥ä¾¿ä¼˜åŒ–è¿‡æ»¤é€»è¾‘\n",
    "        \n",
    "    def _remove_emojis(self, text): # ç§»é™¤emojiï¼Œä¸éœ€è¦æ‰‹åŠ¨\n",
    "        return emoji.replace_emoji(text,replace='')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # äºŒ --------------------------------------------------------------\n",
    "    def _pre_process(self): # æ–‡ä»¶é¢„å¤„ç†æˆä¸€ä¸ªtxtæ–‡æœ¬ï¼Œæ–¹ä¾¿è®­ç»ƒæ¨¡å‹ç›´æ¥ä½¿ç”¨\n",
    "        words_path = self.cache_path\n",
    "        # å…å»å¤„ç†å·²æœ‰æ–‡ä»¶ï¼Œä¸€æ¬¡å¤„ç†\n",
    "        if os.path.exists(self.cache_path):\n",
    "            return words_path\n",
    "\n",
    "        stopset = set([  # ç”¨é›†åˆåŠ é€ŸæŸ¥è¯¢\n",
    "            'ï¼Œ','ã€','ã€‘','#','s','1','2','3','4','5','6','7','8','9','MIUI','@','_','(',')','ï¿£','ï¼','[',']','-',\n",
    "            'â€œ','â€','p','ID',':','ï¼š','~','td','t','d','//','/','ï¼Ÿ','ã€‚','%','>>','>','ã€Š','ã€‹','ã€','+','ing','TNND','ã€œ',\n",
    "            'ğŸ‚','âŠ™','o','(âŠ™oâŠ™)','âŠ™oâŠ™','.','..','..........','â„ƒ','H','â€¦â€¦','^','*','$','ï¿¥','â†—','â†–','Ï‰','â†–(^Ï‰^)â†—','ğŸ˜ƒ','â€¦','\"\"','\"','Y',\n",
    "            'http','cn','com','!',',','çš„','äº†','ï½','...','åœ¨','.','ï¼›',';','â—†','ã€Œ','ã€','##','?','âˆ€','Â·','ï½²','&','---',',','â‰§','â–½','â‰¦','`',\n",
    "            '(',')','ï¼ˆ','ï¼‰'\n",
    "        ])\n",
    "\n",
    "        chunks = pd.read_csv(\n",
    "            self.file_path,\n",
    "            sep='\\t',\n",
    "            header=0,\n",
    "            names=['text'],\n",
    "            chunksize=10000,\n",
    "            usecols=[1],\n",
    "            dtype={'text': str},\n",
    "            engine='c',\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "\n",
    "        words = []\n",
    "        jieba.enable_parallel(4)  # ä½¿ç”¨ 4 ä¸ª CPU æ ¸å¿ƒ\n",
    "        for chunk in tqdm(chunks, desc=\"Processing Text\"):\n",
    "            texts = set(chunk['text'].dropna())  # ç”¨ set() ä»£æ›¿ unique()\n",
    "            # é™¤å»url å’Œè¡¨æƒ…ç¬¦\n",
    "            cleaned_texts = []\n",
    "            for text in texts:\n",
    "                # å»é™¤URL\n",
    "                text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "                # å»é™¤ç‰¹æ®Šç¬¦å·å’Œè¡¨æƒ…ç¬¦å·\n",
    "                text = re.sub(r'\\[.*?\\]', '', text)\n",
    "                # å»é™¤emoji\n",
    "                text = self._remove_emojis(text)\n",
    "                cleaned_texts.append(text)\n",
    "            words.extend([\n",
    "                [w for w in jieba.lcut(str(text)) if w not in stopset]\n",
    "                for text in cleaned_texts\n",
    "            ])\n",
    "        with open(self.cache_path, 'w', encoding='utf-8')as f:\n",
    "            for sentence in words:\n",
    "                if not sentence:  # è·³è¿‡ç©ºå¥å­\n",
    "                    continue\n",
    "                f.write(' '.join(sentence)+'\\n')\n",
    "        print(\"Cache file saved.\")        \n",
    "        self.processed_words = words\n",
    "        return words_path # cache_path   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ä¸‰ --------------------------------------------------------------\n",
    "    def _get_word2vec_model(self):\n",
    "        if os.path.exists(self.model_path):\n",
    "            print(f\"Loading existing model from {self.model_path}\")\n",
    "            return Word2Vec.load(self.model_path)\n",
    "        \n",
    "        words_path = self._pre_process()\n",
    "        if not words_path:  # é¿å…è®­ç»ƒç©ºæ¨¡å‹\n",
    "            raise ValueError(\"No valid words to train the model.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "        \n",
    "        # ä½¿ç”¨LineSentenceè¯»å–å¤§å‹æ–‡æœ¬æ–‡ä»¶(é€è¡Œè¯»å–ï¼Œé¿å…å†…å­˜ä¸è¶³)\n",
    "        sentences = LineSentence(words_path)\n",
    "        \n",
    "        model = Word2Vec(\n",
    "            #list(tqdm(words_list, desc=\"Training Word2Vec\")),#ä½¿ç”¨ tqdm æ˜¾ç¤º Word2Vec è®­ç»ƒè¿›åº¦\n",
    "            #sentences=tqdm(words_list,desc=\"Training Word2Vec\", total=len(words_list)),\n",
    "            sentences=sentences,\n",
    "            vector_size=self.len_vec,\n",
    "            window=self.size_window,\n",
    "            min_count=5,\n",
    "            workers=8,          # ä½¿ç”¨å¤šçº¿ç¨‹\n",
    "            sg=1,               # ä½¿ç”¨skip-gramæ¨¡å‹(1), 0è¡¨ç¤ºCBOW\n",
    "            epochs=50,          # å¯é€‰ï¼šè®¾ç½®è®­ç»ƒè½®æ•°10    \n",
    "            #compute_loss=True,  # å¯é€‰ï¼šæ˜¾ç¤ºè®­ç»ƒæŸå¤±\n",
    "            hs=0,               # ä½¿ç”¨è´Ÿé‡‡æ ·\n",
    "            negative=5,          # è´Ÿé‡‡æ ·æ•°é‡\n",
    "            sample=1e-3         # é«˜é¢‘è¯ä¸‹é‡‡æ ·é˜ˆå€¼\n",
    "        )\n",
    "        model.save(self.model_path)\n",
    "        # ä¿å­˜è¯å‘é‡(çº¯æ–‡æœ¬æ ¼å¼ï¼Œå¯ç”¨äºå…¶ä»–å·¥å…·)\n",
    "        model.wv.save_word2vec_format('/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.vec', binary=False)\n",
    "        \n",
    "        end_time = time.time() # æ—¶é—´æˆ³\n",
    "        print(f\"è®­ç»ƒå®Œæˆ! è€—æ—¶: {end_time-start_time:.2f}ç§’\")\n",
    "        print(f\"æ¨¡å‹å·²ä¿å­˜è‡³: {self.model_path}\")\n",
    "        print(f\"è¯å‘é‡å·²ä¿å­˜è‡³: /Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.vec\")\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # å›› --------------------------------------------------------------\n",
    "    def get_similar_words(self, word):\n",
    "        model = self._get_word2vec_model()\n",
    "        return model.wv.most_similar(word, topn=10)\n",
    "    def get_least_similar_words(self,word):\n",
    "        model = self._get_word2vec_model()\n",
    "        return model.wv.most_similar(word, topn=10, negative=word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # äº” --------------------------------------------------------------\n",
    "    def vis_word_tsne(self,input_word):\n",
    "        model=self._get_word2vec_model()\n",
    "        most_similar = model.wv.most_similar(input_word,topn=10)\n",
    "        least_similar = model.wv.most_similar(input_word,topn=10,negative=input_word)\n",
    "        \n",
    "        #       np.array([model.wv[word] for word, similarity in most_similar + least_similar])\n",
    "        vectors=np.array([model.wv[w]for w,similarity in most_similar+least_similar])\n",
    "        w_list = [w for w, similarity in most_similar+least_similar]\n",
    "        # ä½¿ç”¨t-SNEç®—æ³•å¯¹è¯å‘é‡è¿›è¡Œé™ç»´\n",
    "        tsne = TSNE(n_components=2, perplexity=15)\n",
    "        vectors_tsne = tsne.fit_transform(vectors)\n",
    "        # å¯è§†åŒ–é™ç»´åçš„è¯å‘é‡\n",
    "        #fonts\n",
    "        STH = FontProperties(fname='/System/Library/Fonts/STHeiti Light.ttc')\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(word, fontproperties = STH)\n",
    "        # t-SNEé™ç»´è‡³2ç»´\n",
    "        ax.scatter(vectors_tsne[:10, 0], vectors_tsne[:10, 1], color='purple',label = 'most_simi')\n",
    "        ax.scatter(vectors_tsne[10:, 0], vectors_tsne[10:, 1], color='orange', label = 'least_simi')\n",
    "        ax.legend()\n",
    "        # æ‰“å°è¯\n",
    "        for i, w in enumerate(w_list):\n",
    "            ax.annotate(w, (vectors_tsne[i, 0], vectors_tsne[i, 1]),fontproperties = STH)\n",
    "        plt.show()    \n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text_ana = TextAnalyzer(\n",
    "        file_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo.txt',\n",
    "        model_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo_word2vec.model',\n",
    "        len_vec=200,\n",
    "        size_window=5,\n",
    "        cache_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/processed_words.txt'\n",
    "    )\n",
    "    word='æ—…è¡Œ'\n",
    "    print(f\"å’Œ{word}æœ€ç›¸ä¼¼çš„10ä¸ªè¯ï¼š\",text_ana.get_similar_words(word))\n",
    "    print(f\"å’Œ{word}æœ€ä¸ç›¸ä¼¼çš„10ä¸ªè¯ï¼š\",text_ana.get_least_similar_words(word))\n",
    "    text_ana.vis_word_tsne(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
